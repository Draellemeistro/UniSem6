
| Excercise                 | prev. lecture                                         | next lecture                                                           |
| ------------------------- | ----------------------------------------------------- | ---------------------------------------------------------------------- |
| [[ML AI Modul6 Exercise]] | [[MachineLearning AI Modul 5\|Unsupervised Learning]] | [[MachineLearning AI Modul 7\|7 Multilayer Artificial Neural Network]] |

# Data preprocessing and Hyperparameter Tuning
## Dealing with missing data
Samples could be missing one or more values for various reasons

Most computational tools are unable to handle such missing values, or produce unpredictable result if we simply ignore them
- **Eliminate the samples or features with missing values**
	- ![[image files etc/ML images/MachineLearning AI Modul 6.png]]

**==However, there is a problem when we eliminate rows or columns. What kind of problem? How to solve it?==**

## Imputing Missing Values
- the removal of samples or dropping of entire feature columns is simply not feasible, because **we might lose too much valuable data**
- In this case, we can use different interpolation techniques to estimate the missing values from the other training samples in our dataset
- One of the most common interpolation techniques is mean imputation, where we simply **replace the missing value with the mean value of the entire feature column**

+ If we change **the axis=0 setting to axis=1, we'd calculate the row means**
+ Other options for the strategy parameter are median most_frequent, where **the latter replaces the missing values with the most frequent values**.
+ This is useful for **inputting categorical feature** values, for example, a feature column that stores **an encoding of color names, such as red, green, and blue**
<font style="color:green">Text Color</font>
<mark class="hltr-pink">sss</mark>

## Handling Categorical Data

> [!NOTE] **Categorical Data:**
> Nominal and Ordinal features, e.g.:
> **Color, Size, Price, classlabel**
> 1. gren, M, 10.1, class1
> 2. red, L, 13.5, class2
> 3. blue, XL, 15.3 class1
> 
> vvv bliver til vvvv
> $XL=L+1=M+2$
> 1. gren, 1, 10.1, class1
> 2. red, 2, 13.5, class2
> 3. blue, 3, 15.3 class1

### Encoding Class Labels
- Many machine learning librarires require that class labels are encoded as integer values.
- Although most estimators for classification in scikit-learn convert class labels to integers internally, it is considered good practice to provide class labels as integer arrays to avoud technical glitches

```python
>>> import numpy as np
>>> class_mapping = {label:idx for idx, label in
...             enumerate (np.unique (df ['classlabel'] ) ) ]

>>> class_mapping
{'class1': 0, 'class2': 1}
```

```python
>>> df ['classlabel'] = df ['classlabel' ] .map (class_mapping)
>>> df
	color  size  price  classlabel
0	green   1    10.1       0
1	red     2    13.5       1
2	blue    3    15.3       0
```

### Performing one-hot encoding on nominal features
```python
>>> X = df [ ['color', 'size', 'price'] ]. values
>>> color_le = LabelEncoder ()
>>> X[:, 0] = color_le.fit_transform(X[:, 0] )
>>> X
array([[1, 1, 10.1],
	   [2, 2, 13.5],
	   [0, 3, 15.3]], dtype=object)
```

```python
>>> pd.get_dummies (df [ ['price', 'color', 'size' ] ] )
	price size color_blue color_green color_red
0   10.1   1        0          1           0
1   13.5   2        0          0           1
2   15.3   3        1          0           0
```

### Feature Scaling

> [!NOTE] Title
> - Normalization
> 	- Normalization is also known as **min-max normalization** or **min-max scaling**
> 	- Normalization re-scales values in the range of  0-1
> - Standardization
> 	- Standardization is also known as **z-score Normalization**
> 	- in Standardization, features are scaled to have **zero-mean and one-standard-deviation.** It means af standardization, features wil have **mean = 0 and standard deviation = 1**

![[MachineLearning AI Modul 6 1.png]]
![[MachineLearning AI Modul 6-1.png]]

### Feature Selection
- An alternative way to reduce the complexity of the model and avoid overfitting is dimensionality reduction via feature selection, which is especially useful for unregularized models
- there are two main categories of dimensionality reduction techniques:
	- **feature selection**
	- **feature extraction**

#### Sequential Backward Selection (SBS)
1. Initialize the algorithm with k=d, where d is the dimensionality of the full feature space $X_{d}$
2. Determine the feature $x^{-}$ that maximizes the criterion: $x^{-} = \text{argmax } J (X_{k}-x))$ where $x \in X_{k}$
3. Remove the feature $x^{-}$ from the feature set: $X_{k-1}=X_{k}-x^{-};k=k-1$
4. Terminate if k equals the number of desired features; otherwise, go to step 2.
![[MachineLearning AI Modul 6-2.png]]

### Feature Extraction
#### Principal Component Analysis (PCA)
**For unsupervised data compression**
![[MachineLearning AI Modul 6-3.png]]
#### Linear Discriminant Analysis (LDA)
**As a supervised dimensionality reduction technique for maximizing class reparability**







### Total and explained variance
$$\frac{\lambda_{j}}{\sum_{j=1}^{d}\lambda_{j}}$$
$\lambda_{j}$ -- eigenvalue
![[MachineLearning AI Modul 6-7.png]]

#### Linear Discriminant Analysis
1. Standardize the d-dimensional dataset (d is the number of features)
2. For each class, compute the d-dimensional mean vector
3. Construct the between-class scatter matrix $S_{B}$ and the within-class scatter matrix $S_{W}$
4. Compute the eigenvectors and corresponding eigenvalues of the matrix $S_{W}^{-1}S_{B}$
5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors
6. Choose the N eigenvectors that correspond to the N largest eigenvalues to construct a d k x -dimensional transformation matrix W; the eigenvectors are the columns of this matrix
![[MachineLearning AI Modul 6-8.png]]

## [[Hyperparameter Tuning]]
