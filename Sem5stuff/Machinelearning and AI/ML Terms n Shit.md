# asdasd
## Basic Terminology and Notations
- ML :
- AI :
	- General AI
	- Narrow AI
	- ss
- [[Data Preprocessing]]
	- labels 
	- features
	- Datasets
		- Training set
		- Test set
- [[Learning Algorithm]]
	- Examples:
		- **[[Classification]]**
			- [[Perceptron]] ????? MCP (*McCullock-Pitts Neuron*)
			- [[Adaline]]
			- [[logistic regression]]
			- [[Support Vector Machines]]
			- [[Decision tree classifiers]]
	- Model Selection
	- Cross-validation
	- Hyperparameter Optimization
- Evaluation Final Model
- problems or difficulties
	- [[regularization]]
		- [[overfitting and underfitting]]
	- bias and variance problems with learning curves
## Data Preprocessing
- [[Data Preprocessing]]
	-  [[Feature Scaling]]
	- [[Dimension reduction]]
		- [[Feature Extraction]] / data cleaning
		- Feature Selection
		- Sampling???
- Splitting into Training and Test sets
## Types of ML
- Supervised Learning
- Semi-supervised learning
- unsupervised learning
- reinforcement learning
	- Q-learning


## Somewhere ????
- [[Classification]]
- 
## Clustering
- [[K-means Clustering]]
	- K-means++
- [[Hierarchical Clustering]]
	- Agglomerative  Clustering
		- Single linkage
		- Complete linkage
	- some other clustering


## Hyper

# start til slut
## 1.
ss
## 2. Simple ML algorithms for [[Classification]]
- Perceptro
- Adaline

- Standardization
- Normalization
## 3. Powerful ML algorithms for [[Classification]] (part 1)
- [[logistic regression]]
	- the logistic cost function
	- Tackling [[overfitting and underfitting]] via regularization
- [[Support Vector Machines]]
	- Kernel Support Vector Machines
## 3. Powerful ML algorithms for [[Classification]] (part 2)
- [[Decision tree classifiers]]
	- impurity measures
		- Entropy
		- Gini impurity
		- Classification Error
	- Information Gain (IG)
	- classification trees
	- regression trees (*i guess if we want a continuous variable as output?*)
	- **other trees???**
- One-hot [[Encoding]]
- [[Random Forest]]
- [[k-nearest neighbor (KNN)]]
## 4.
- all the [[Random Forest]] and [[Decision tree classifiers]] shit
- sampling with replacement
- 
## 5. Unsupervised Learning
- [[clustering]] 
	- [[Classification]] vs [[clustering]] ([[Supervised Learning]] vs [[Unsupervised Learning]])
	- [[K-means Clustering]]
		- K-means++
	- Hard vs Soft Clustering
	- Elbow method
		- [[Hyperparameter Tuning]]
	- Hierarchical clustering
		- Agglomerative
			- single linkage
			- full linkage
		- divisive
	- [[DBSCAN]] Density-Based Spatial Clustering of Applications with Noise
## 6. [[Data Preprocessing]] and [[Hyperparameter Tuning]]
- [[Data Preprocessing]]
	- [[Imputing Missing Values]]
		- Strategy parameter
			- Mean imputation
			- median 
			- most_frequent
	- Handling Categorical Data
		- [[Encoding]] class labels
		- one-hot [[Encoding]] on nominal features
	- [[Feature Scaling]]
		- Normalization
		- Standardization
	- [[Feature Selection]]
		- Sequential Backward Selection (SBS)
	- [[Feature Extraction]]
		- Principal Component Analysis (PCA)
		- Linear Discriminant Analysis (LDA)
- [[Hyperparameter Tuning]]
	- Holdout method
	- K-Fold Cross-Validation
	- Nested Cross-Validation
	- Diagnosing bias and variance problems with learning curves
	- [[Performance Metrics]]
		- Confusion matrix
		- precision
		- recall
		- F1-score
## 7.
- [[Multilayer Artificial Neural Networks]]
	- multilayer feedforward neural network: [[Multilayer Perceptron (MLP)]]
	- 
- parallelizing neural network training with tensorflow
## 8.
ss
## 9.
ss
## 10.
ss